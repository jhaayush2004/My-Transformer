{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd55afcc",
   "metadata": {},
   "source": [
    "# Self Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb491e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c21e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "L,d_k,d_v,d_q=4,8,8,8\n",
    "q=np.random.randn(L, d_q)\n",
    "k=np.random.randn(L, d_k)\n",
    "v=np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba7f158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query\n",
      " [[-0.64701818 -0.0558079  -1.14771189  0.49863223 -1.88128067  1.25783586\n",
      "   0.04549632 -0.29669845]\n",
      " [ 0.93365342  1.20111119 -0.88908306 -0.50505876 -0.06488743 -0.87293153\n",
      "  -0.9846306  -0.18188075]\n",
      " [ 0.18629219 -0.96036127  0.70557281  0.2958023  -0.33001171  0.94506385\n",
      "  -0.13190045  0.28534063]\n",
      " [-0.60255302  1.45869212 -0.63609783 -0.28848135 -0.47130473 -1.36020655\n",
      "  -0.78698731 -0.68169012]]\n",
      "Key\n",
      " [[-0.24554501 -0.65990683  2.15165207 -0.94649725  0.72228101 -0.02113326\n",
      "  -0.98827148 -0.24726241]\n",
      " [ 0.6892279   0.59099566  1.32857133 -0.22630307 -0.1476044  -0.60683591\n",
      "  -0.02796739 -1.32775192]\n",
      " [-1.07703429 -0.378611    1.73588861  0.29028997 -0.74374386 -0.05225929\n",
      "   2.00015962 -0.16328197]\n",
      " [ 0.38265766 -0.63299722 -1.49036348  0.01767426  1.30787406 -0.26752549\n",
      "   1.10496892 -0.62999402]]\n",
      "Value\n",
      " [[-0.6714164  -0.26395483 -0.94671032  1.02257215 -0.31060232 -0.1417706\n",
      "  -0.74010142 -2.35063822]\n",
      " [-0.89597015  0.72128891 -0.37797554 -0.49751407 -0.75425701  1.55382614\n",
      "   0.11007087 -2.56203123]\n",
      " [ 1.63904376 -0.49702886  1.19788858  0.72699062  0.98882034  1.62989935\n",
      "   1.03421495  1.7043243 ]\n",
      " [ 1.5949958  -0.58582972  0.85519434 -0.56700781  0.22704864  1.78734974\n",
      "  -1.08951998 -0.66644487]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query\\n\",q)\n",
    "print(\"Key\\n\",k)\n",
    "print(\"Value\\n\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23560e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.10272639, -2.20952946,  0.34334095, -1.05273045],\n",
       "       [-1.46720055,  1.09477152, -4.9961354 ,  0.0883644 ],\n",
       "       [ 1.62764295, -0.46866889,  1.3592674 , -1.37709094],\n",
       "       [-1.27561681,  1.48908142, -2.13242393, -0.90365444]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ba631d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6248391862466455, 0.858228453721279, 3.428832799627412)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa8df07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6248391862466455, 0.858228453721279, 0.4286040999534264)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T)/math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6ad6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.45053283, -0.78118663,  0.12138936, -0.37219642],\n",
       "       [-0.51873373,  0.38706018, -1.76640061,  0.03124153],\n",
       "       [ 0.57545868, -0.16569948,  0.4805736 , -0.48687517],\n",
       "       [-0.45099865,  0.52646978, -0.75392571, -0.31949009]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af3e18e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril((np.ones((L,L))))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a92432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0]=-np.infty\n",
    "mask[mask==1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f98417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71623de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.45053283,        -inf,        -inf,        -inf],\n",
       "       [-0.51873373,  0.38706018,        -inf,        -inf],\n",
       "       [ 0.57545868, -0.16569948,  0.4805736 ,        -inf],\n",
       "       [-0.45099865,  0.52646978, -0.75392571, -0.31949009]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled+mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c44ede49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33415266",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f28ead8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.2878613 , 0.7121387 , 0.        , 0.        ],\n",
       "       [0.41910462, 0.19972919, 0.38116619, 0.        ],\n",
       "       [0.18060582, 0.47999961, 0.13340502, 0.20598955]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bb8a3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6714164 , -0.26395483, -0.94671032,  1.02257215, -0.31060232,\n",
       "        -0.1417706 , -0.74010142, -2.35063822],\n",
       "       [-0.83132982,  0.43767536, -0.54169228, -0.05994007, -0.62654599,\n",
       "         1.06572945, -0.13466083, -2.50117936],\n",
       "       [ 0.16440296, -0.15601284, -0.01566879,  0.60630087,  0.09608288,\n",
       "         0.87219026,  0.10601222, -0.84724495],\n",
       "       [-0.0041179 ,  0.11156567, -0.01644406, -0.07393756, -0.23945641,\n",
       "         1.30584347, -0.1672929 , -1.56422819]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v =  np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d7ab0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6714164 , -0.26395483, -0.94671032,  1.02257215, -0.31060232,\n",
       "        -0.1417706 , -0.74010142, -2.35063822],\n",
       "       [-0.89597015,  0.72128891, -0.37797554, -0.49751407, -0.75425701,\n",
       "         1.55382614,  0.11007087, -2.56203123],\n",
       "       [ 1.63904376, -0.49702886,  1.19788858,  0.72699062,  0.98882034,\n",
       "         1.62989935,  1.03421495,  1.7043243 ],\n",
       "       [ 1.5949958 , -0.58582972,  0.85519434, -0.56700781,  0.22704864,\n",
       "         1.78734974, -1.08951998, -0.66644487]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa03f17",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64b114f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages (0.15.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages (2.0.2+cu117)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\ayush\\appdata\\roaming\\python\\python310\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1cb5402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8c70970",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=4\n",
    "batch_size=1\n",
    "input_dim=512\n",
    "d_model=512 #output of attention model for every single word\n",
    "x = torch.randn( (batch_size,sequence_length,input_dim) ) #sample input we get after as output from attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ec3256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcb8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer=nn.Linear(input_dim, 3*d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ad7880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv=qkv_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04fb0ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9db563fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'qkv distribution')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1,1,0.01)*3\n",
    "plt.bar(x_val,y_val,align='center',color=['forestgreen'])\n",
    "plt.title('qkv distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ceca11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "head_dim=d_model//num_heads\n",
    "qkv = qkv.reshape(batch_size,sequence_length,num_heads,3*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3cb3321e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2cffe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0,2,1,3)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1adc37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k ,v = qkv.chunk(3,dim=-1)\n",
    "q.shape,k.shape,v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31642799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]\n",
    "scaled = torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c39fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\AppData\\Local\\Temp\\ipykernel_17116\\3717780648.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  k.T.shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 8, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bbff9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask =torch.full(scaled.size(),float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "373d40fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3911,    -inf,    -inf,    -inf],\n",
       "          [ 0.0577, -0.4869,    -inf,    -inf],\n",
       "          [-0.5566, -0.5639, -0.1631,    -inf],\n",
       "          [-0.0058,  0.2288,  0.4268,  0.0592]],\n",
       "\n",
       "         [[ 0.0572,    -inf,    -inf,    -inf],\n",
       "          [ 0.0606,  0.3213,    -inf,    -inf],\n",
       "          [-0.0271, -0.2859, -0.3119,    -inf],\n",
       "          [ 0.2995,  0.2023,  0.0307,  0.2783]],\n",
       "\n",
       "         [[-0.1047,    -inf,    -inf,    -inf],\n",
       "          [ 0.2869,  0.5947,    -inf,    -inf],\n",
       "          [ 0.4800, -0.3883,  0.2293,    -inf],\n",
       "          [-0.3642, -0.4278,  0.0478, -0.5409]],\n",
       "\n",
       "         [[ 0.0629,    -inf,    -inf,    -inf],\n",
       "          [ 0.1743,  0.5814,    -inf,    -inf],\n",
       "          [ 0.3219,  0.1106, -0.2197,    -inf],\n",
       "          [-0.2296, -0.1188,  0.2800, -0.3095]],\n",
       "\n",
       "         [[ 0.4911,    -inf,    -inf,    -inf],\n",
       "          [ 0.1320,  0.6294,    -inf,    -inf],\n",
       "          [-0.1384, -0.1401,  0.0785,    -inf],\n",
       "          [ 0.3381, -0.0686,  0.3475, -0.0621]],\n",
       "\n",
       "         [[ 0.2449,    -inf,    -inf,    -inf],\n",
       "          [ 0.0201,  0.3276,    -inf,    -inf],\n",
       "          [ 0.2788, -0.0392, -0.0030,    -inf],\n",
       "          [-0.3223,  0.2007,  0.2711,  0.1092]],\n",
       "\n",
       "         [[ 0.5525,    -inf,    -inf,    -inf],\n",
       "          [-0.3161,  0.0519,    -inf,    -inf],\n",
       "          [-0.2133,  0.4705,  0.3471,    -inf],\n",
       "          [-0.1154,  0.1148, -0.4345,  0.6942]],\n",
       "\n",
       "         [[ 0.1740,    -inf,    -inf,    -inf],\n",
       "          [-0.1379,  0.5621,    -inf,    -inf],\n",
       "          [ 0.2341,  0.4961,  0.1851,    -inf],\n",
       "          [-0.0592,  0.6346,  0.1404, -0.4583]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b17ab56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled+=mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a03ea62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3204900",
   "metadata": {},
   "outputs": [],
   "source": [
    " attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "971e357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbd8b460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6329, 0.3671, 0.0000, 0.0000],\n",
       "        [0.2878, 0.2857, 0.4265, 0.0000],\n",
       "        [0.2052, 0.2595, 0.3163, 0.2190]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "799559c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention,v)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cf246b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d75384a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product(q, k, v, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "521e8104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b886f7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6329, 0.3671, 0.0000, 0.0000],\n",
       "        [0.2878, 0.2857, 0.4265, 0.0000],\n",
       "        [0.2052, 0.2595, 0.3163, 0.2190]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e73dc863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ca6a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values=values.reshape(batch_size,sequence_length,num_heads*head_dim)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e18a7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(d_model,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8f65b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8f47ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=linear_layer(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa5fd7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6112bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim=input_dim\n",
    "        self.d_model=d_model\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=d_model//num_heads\n",
    "        self.qkv_layer=nn.Linear(input_dim,3*d_model)\n",
    "        self.linear_layer=nn.Linear(d_model,d_model)\n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6658f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q size: torch.Size([30, 8, 5, 64]), k size: torch.Size([30, 8, 5, 64]), v size: torch.Size([30, 8, 5, 64]), \n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size:torch.Size([30, 8, 5, 5]) \n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim=1024\n",
    "d_model=512\n",
    "num_heads=8\n",
    "batch_size=30\n",
    "sequence_length=5\n",
    "x = torch.randn((batch_size,sequence_length, input_dim))\n",
    "model = MultiheadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c77917",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "421f899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "947091e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=10\n",
    "d_model=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a479b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6b8442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i = torch.arange(0,d_model,2).float()\n",
    "even_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34bc299f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  10.0000, 100.0000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_denominator=torch.pow(1000,even_i/d_model)\n",
    "even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c35bc140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_i = torch.arange(1,d_model,2).float()\n",
    "odd_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d3653bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  10.0000, 100.0000])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_denominator=torch.pow(1000,(odd_i-1)/d_model)\n",
    "odd_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "606fc878",
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "216cabc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a62f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = torch.arange(max_seq_len, dtype=torch.float).reshape(max_seq_len,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8efcfb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31e32f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_PE=torch.sin(position/denominator)\n",
    "odd_PE=torch.cos(position/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee36baf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0998,  0.0100],\n",
       "        [ 0.9093,  0.1987,  0.0200],\n",
       "        [ 0.1411,  0.2955,  0.0300],\n",
       "        [-0.7568,  0.3894,  0.0400],\n",
       "        [-0.9589,  0.4794,  0.0500],\n",
       "        [-0.2794,  0.5646,  0.0600],\n",
       "        [ 0.6570,  0.6442,  0.0699],\n",
       "        [ 0.9894,  0.7174,  0.0799],\n",
       "        [ 0.4121,  0.7833,  0.0899]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9097f1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb375c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5403,  0.9950,  0.9999],\n",
       "        [-0.4161,  0.9801,  0.9998],\n",
       "        [-0.9900,  0.9553,  0.9996],\n",
       "        [-0.6536,  0.9211,  0.9992],\n",
       "        [ 0.2837,  0.8776,  0.9988],\n",
       "        [ 0.9602,  0.8253,  0.9982],\n",
       "        [ 0.7539,  0.7648,  0.9976],\n",
       "        [-0.1455,  0.6967,  0.9968],\n",
       "        [-0.9111,  0.6216,  0.9960]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "31dd8572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2d0a9b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 2])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked =torch.stack([even_PE, odd_PE],dim=2)\n",
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2e39e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0998,  0.9950,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.1987,  0.9801,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.2955,  0.9553,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.3894,  0.9211,  0.0400,  0.9992],\n",
       "        [-0.9589,  0.2837,  0.4794,  0.8776,  0.0500,  0.9988],\n",
       "        [-0.2794,  0.9602,  0.5646,  0.8253,  0.0600,  0.9982],\n",
       "        [ 0.6570,  0.7539,  0.6442,  0.7648,  0.0699,  0.9976],\n",
       "        [ 0.9894, -0.1455,  0.7174,  0.6967,  0.0799,  0.9968],\n",
       "        [ 0.4121, -0.9111,  0.7833,  0.6216,  0.0899,  0.9960]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = torch.flatten(stacked, start_dim=1,end_dim=2)\n",
    "PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22fa0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8bbdd310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(d_model=6, max_sequence_length=10)\n",
    "pe.forward()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f301b41",
   "metadata": {},
   "source": [
    "# Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6b497fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3175861e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([[[0.2,0.1,0.3],[0.5,0.1,0.1]]])\n",
    "B, S, E = inputs.size()\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c7d70742",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_shape = inputs.size()[-2:]\n",
    "gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameter_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bcb2addb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.size(),beta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c1ecdfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [-(i+1) for i in range(len(parameter_shape))] #computing dim. for which we want to compute layer norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7fd93619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "773f2f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(dim=dims,keepdim=True)\n",
    "mean.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2753711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ((inputs-mean)**2).mean(dim=dims,keepdim=True)\n",
    "epsilon=1e-5\n",
    "std = (var+epsilon).sqrt()\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3fa7485b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=(inputs-mean)/std\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f396f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=gamma*y+beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c6773ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "25ce8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNormalization():\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        print(f\"Mean \\n ({mean.size()}): \\n {mean}\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation \\n ({std.size()}): \\n {std}\")\n",
    "        y = (inputs - mean) / std\n",
    "        print(f\"y \\n ({y.size()}) = \\n {y}\")\n",
    "        out = self.gamma * y  + self.beta\n",
    "        print(f\"out \\n ({out.size()}) = \\n {out}\")\n",
    "        return out\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4bca29e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input \n",
      " (torch.Size([5, 3, 8])) = \n",
      " tensor([[[-2.3358, -0.8349,  0.6129,  0.2221,  0.8131,  0.3440,  1.1632,\n",
      "          -0.7677],\n",
      "         [ 0.4960, -0.5228,  0.1700, -0.6283,  0.5292, -1.1288, -1.4803,\n",
      "          -0.5075],\n",
      "         [-0.7110, -0.8505, -0.1684, -0.9838, -0.1590,  0.8612,  0.5280,\n",
      "          -0.0668]],\n",
      "\n",
      "        [[ 0.8936,  0.1976,  0.0652, -0.3803,  0.1890,  0.0130,  0.5320,\n",
      "           0.2924],\n",
      "         [ 0.8288,  0.4098,  1.1854,  1.7668, -0.4285,  0.0101,  0.5946,\n",
      "          -0.1522],\n",
      "         [-0.5293, -0.0314,  0.9387,  1.1959,  1.2019,  0.3938, -1.1813,\n",
      "           0.1116]],\n",
      "\n",
      "        [[ 0.6286, -0.3239,  0.8896, -1.4750,  0.7463, -0.3977,  0.6681,\n",
      "          -1.4397],\n",
      "         [-0.4544, -0.2757,  0.8730,  0.3426,  0.9533,  1.1710,  0.0803,\n",
      "          -0.9873],\n",
      "         [-0.7794,  1.4728,  0.0252, -3.2549, -0.6518,  1.8694,  0.7618,\n",
      "          -0.0886]],\n",
      "\n",
      "        [[ 0.3141,  0.5907, -1.5755, -0.4741,  0.8899, -0.0818,  0.9707,\n",
      "           1.3888],\n",
      "         [-0.2247,  1.8296,  1.2974, -0.5298,  0.1110,  1.6472, -0.1117,\n",
      "           0.8817],\n",
      "         [-0.2463,  0.4556, -1.6656,  0.1218, -1.2683,  0.7435, -0.1065,\n",
      "           0.3602]],\n",
      "\n",
      "        [[-0.7122,  1.2095,  0.4122, -0.8968, -0.0036, -0.1932,  0.0544,\n",
      "           0.3614],\n",
      "         [-0.6749, -1.0921,  0.9409, -1.8879, -0.9607, -1.0605,  0.8238,\n",
      "           0.5510],\n",
      "         [-0.4263,  0.6756,  0.0879,  0.7824, -0.9806,  0.7852,  0.3699,\n",
      "          -1.9322]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "sentence_length = 5\n",
    "embedding_dim = 8 \n",
    "inputs = torch.randn(sentence_length, batch_size, embedding_dim)\n",
    "\n",
    "print(f\"input \\n ({inputs.size()}) = \\n {inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a31263bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.9736e-08, grad_fn=<MeanBackward0>),\n",
       " tensor(1.2238, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out[0].mean(), out[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab543e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a814d78",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cc8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42775203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    print(f\"scaled.size() : {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
    "        # Broadcasting add. So just the last N dimensions need to match\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c332aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_sequence_length, d_model = x.size()\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\n",
    "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "        out = self.linear_layer(values)\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f11bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        print(f\"Mean ({mean.size()})\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation  ({std.size()})\")\n",
    "        y = (inputs - mean) / std\n",
    "        print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y  + self.beta\n",
    "        print(f\"self.gamma: {self.gamma.size()}, self.beta: {self.beta.size()}\")\n",
    "        print(f\"out: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fd22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x)\n",
    "        print(f\"x after activation: {x.size()}\")\n",
    "        x = self.dropout(x)\n",
    "        print(f\"x after dropout: {x.size()}\")\n",
    "        x = self.linear2(x)\n",
    "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4d3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_x = x\n",
    "        print(\"------- ATTENTION 1 ------\")\n",
    "        x = self.attention(x, mask=None)\n",
    "        print(\"------- DROPOUT 1 ------\")\n",
    "        x = self.dropout1(x)\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        print(\"------- ATTENTION 2 ------\")\n",
    "        x = self.ffn(x)\n",
    "        print(\"------- DROPOUT 2 ------\")\n",
    "        x = self.dropout2(x)\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b8cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                     for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c2bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5\n",
    "\n",
    "encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb6b4ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 1 ------\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv.size(): torch.Size([30, 200, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 200, 192])\n",
      "q size: torch.Size([30, 8, 200, 64]), k size: torch.Size([30, 8, 200, 64]), v size: torch.Size([30, 8, 200, 64]), \n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values.size(): torch.Size([30, 8, 200, 64]), attention.size:torch.Size([30, 8, 200, 200]) \n",
      "values.size(): torch.Size([30, 200, 512])\n",
      "out.size(): torch.Size([30, 200, 512])\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "------- ATTENTION 2 ------\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after activation: torch.Size([30, 200, 2048])\n",
      "x after dropout: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "self.gamma: torch.Size([512]), self.beta: torch.Size([512])\n",
      "out: torch.Size([30, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # includes positional encoding\n",
    "out = encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12242b69",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4f71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    # q: 30 x 8 x 200 x 64, k: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64, mask 200 x 200\n",
    "    d_k = q.size()[-1] \n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k) # 30 x 8 x 200 x 200\n",
    "    print(f\"scaled.size() : {scaled.size()}\")\n",
    "    if mask is not None:\n",
    "        print(f\"-- ADDING MASK of shape {mask.size()} --\") \n",
    "        scaled += mask # 30 x 8 x 200 x 200\n",
    "    attention = F.softmax(scaled, dim=-1) # 30 x 8 x 200 x 200\n",
    "    values = torch.matmul(attention, v) # 30 x 8 x 200 x 64\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  x: 30 x 200 x 512\n",
    "        x = self.linear1(x) #30 x 200 x 2048\n",
    "        print(f\"x after first linear layer: {x.size()}\")\n",
    "        x = self.relu(x) #30 x 200 x 2048\n",
    "        print(f\"x after relu layer: {x.size()}\")\n",
    "        x = self.dropout(x) #30 x 200 x 2048\n",
    "        print(f\"x after dropout layer: {x.size()}\")\n",
    "        x = self.linear2(x) #30 x 200 x 512\n",
    "        print(f\"x after 2nd linear layer: {x.size()}\")\n",
    "        return x #30 x 200 x 512\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape)) # 512\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape)) # 512\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs : 30 x 200 x 512\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))] # [-1]\n",
    "        print(f\"dims: {dims}\")\n",
    "        mean = inputs.mean(dim=dims, keepdim=True) #30 x 200 x 1\n",
    "        print(f\"Mean ({mean.size()})\")\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True) # 30 x 200 x 512\n",
    "        std = (var + self.eps).sqrt() # 30 x 200 x 512\n",
    "        print(f\"Standard Deviation  ({std.size()})\")\n",
    "        y = (inputs - mean) / std # 30 x 200 x 512\n",
    "        print(f\"y: {y.size()}\")\n",
    "        out = self.gamma * y  + self.beta  # 30 x 200 x 512\n",
    "        print(f\"out: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model) # 1536 \n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512 \n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        qkv = self.qkv_layer(x) # 30 x 200 x 1536\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim) # 30 x 200 x 8 x 192\n",
    "        print(f\"qkv after reshape .size(): {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 192\n",
    "        print(f\"qkv after permutation: {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # q: 30 x 8 x 200 x 64, k: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n",
    "        print(f\"q: {q.size()}, k:{k.size()}, v:{v.size()}\")\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) # values: 30 x 8 x 200 x 64\n",
    "        print(f\"values: {values.size()}, attention:{attention.size()}\")\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim) # 30 x 200 x 512\n",
    "        print(f\"values after reshaping: {values.size()}\")\n",
    "        out = self.linear_layer(values) # 30 x 200 x 512\n",
    "        print(f\"out after passing through linear layer: {out.size()}\")\n",
    "        return out # 30 x 200 x 512\n",
    "\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model , 2 * d_model) # 1024\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y, mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size() # 30 x 200 x 512\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "        kv = self.kv_layer(x) # 30 x 200 x 1024\n",
    "        print(f\"kv.size(): {kv.size()}\")\n",
    "        q = self.q_layer(y) # 30 x 200 x 512\n",
    "        print(f\"q.size(): {q.size()}\")\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)  # 30 x 200 x 8 x 128\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)  # 30 x 200 x 8 x 64\n",
    "        kv = kv.permute(0, 2, 1, 3) # 30 x 8 x 200 x 128\n",
    "        q = q.permute(0, 2, 1, 3) # 30 x 8 x 200 x 64\n",
    "        k, v = kv.chunk(2, dim=-1) # K: 30 x 8 x 200 x 64, v: 30 x 8 x 200 x 64\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) #  30 x 8 x 200 x 64\n",
    "        print(f\"values: {values.size()}, attention:{attention.size()}\")\n",
    "        values = values.reshape(batch_size, sequence_length, d_model) #  30 x 200 x 512\n",
    "        out = self.linear_layer(values)  #  30 x 200 x 512\n",
    "        print(f\"out after passing through linear layer: {out.size()}\")\n",
    "        return out  #  30 x 200 x 512\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, y, decoder_mask):\n",
    "        _y = y # 30 x 200 x 512\n",
    "        print(\"MASKED SELF ATTENTION\")\n",
    "        y = self.self_attention(y, mask=decoder_mask) # 30 x 200 x 512\n",
    "        print(\"DROP OUT 1\")\n",
    "        y = self.dropout1(y) # 30 x 200 x 512\n",
    "        print(\"ADD + LAYER NORMALIZATION 1\")\n",
    "        y = self.norm1(y + _y) # 30 x 200 x 512\n",
    "\n",
    "        _y = y # 30 x 200 x 512\n",
    "        print(\"CROSS ATTENTION\")\n",
    "        y = self.encoder_decoder_attention(x, y, mask=None) #30 x 200 x 512\n",
    "        print(\"DROP OUT 2\")  #30 x 200 x 512\n",
    "        y = self.dropout2(y)\n",
    "        print(\"ADD + LAYER NORMALIZATION 2\")\n",
    "        y = self.norm2(y + _y)  #30 x 200 x 512\n",
    "\n",
    "        _y = y  #30 x 200 x 512\n",
    "        print(\"FEED FORWARD 1\")\n",
    "        y = self.ffn(y) #30 x 200 x 512\n",
    "        print(\"DROP OUT 3\")\n",
    "        y = self.dropout3(y) #30 x 200 x 512\n",
    "        print(\"ADD + LAYER NORMALIZATION 3\")\n",
    "        y = self.norm3(y + _y) #30 x 200 x 512\n",
    "        return y #30 x 200 x 512\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, mask) #30 x 200 x 512\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) \n",
    "                                          for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, mask):\n",
    "        #x : 30 x 200 x 512 \n",
    "        #y : 30 x 200 x 512\n",
    "        #mask : 200 x 200\n",
    "        y = self.layers(x, y, mask)\n",
    "        return y #30 x 200 x 512\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46fc5a90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASKED SELF ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
      "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
      "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "values after reshaping: torch.Size([30, 200, 512])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "CROSS ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "kv.size(): torch.Size([30, 200, 1024])\n",
      "q.size(): torch.Size([30, 200, 512])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 2\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "FEED FORWARD 1\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu layer: torch.Size([30, 200, 2048])\n",
      "x after dropout layer: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 3\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "MASKED SELF ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
      "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
      "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "values after reshaping: torch.Size([30, 200, 512])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "CROSS ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "kv.size(): torch.Size([30, 200, 1024])\n",
      "q.size(): torch.Size([30, 200, 512])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 2\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "FEED FORWARD 1\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu layer: torch.Size([30, 200, 2048])\n",
      "x after dropout layer: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 3\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "MASKED SELF ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
      "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
      "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "values after reshaping: torch.Size([30, 200, 512])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "CROSS ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "kv.size(): torch.Size([30, 200, 1024])\n",
      "q.size(): torch.Size([30, 200, 512])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 2\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "FEED FORWARD 1\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu layer: torch.Size([30, 200, 2048])\n",
      "x after dropout layer: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 3\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "MASKED SELF ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
      "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
      "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "values after reshaping: torch.Size([30, 200, 512])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "CROSS ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "kv.size(): torch.Size([30, 200, 1024])\n",
      "q.size(): torch.Size([30, 200, 512])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 2\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "FEED FORWARD 1\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu layer: torch.Size([30, 200, 2048])\n",
      "x after dropout layer: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 3\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "MASKED SELF ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "qkv.size(): torch.Size([30, 200, 1536])\n",
      "qkv after reshape .size(): torch.Size([30, 200, 8, 192])\n",
      "qkv after permutation: torch.Size([30, 8, 200, 192])\n",
      "q: torch.Size([30, 8, 200, 64]), k:torch.Size([30, 8, 200, 64]), v:torch.Size([30, 8, 200, 64])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "-- ADDING MASK of shape torch.Size([200, 200]) --\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "values after reshaping: torch.Size([30, 200, 512])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "CROSS ATTENTION\n",
      "x.size(): torch.Size([30, 200, 512])\n",
      "kv.size(): torch.Size([30, 200, 1024])\n",
      "q.size(): torch.Size([30, 200, 512])\n",
      "scaled.size() : torch.Size([30, 8, 200, 200])\n",
      "values: torch.Size([30, 8, 200, 64]), attention:torch.Size([30, 8, 200, 200])\n",
      "out after passing through linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 2\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n",
      "FEED FORWARD 1\n",
      "x after first linear layer: torch.Size([30, 200, 2048])\n",
      "x after relu layer: torch.Size([30, 200, 2048])\n",
      "x after dropout layer: torch.Size([30, 200, 2048])\n",
      "x after 2nd linear layer: torch.Size([30, 200, 512])\n",
      "DROP OUT 3\n",
      "ADD + LAYER NORMALIZATION 3\n",
      "dims: [-1]\n",
      "Mean (torch.Size([30, 200, 1]))\n",
      "Standard Deviation  (torch.Size([30, 200, 1]))\n",
      "y: torch.Size([30, 200, 512])\n",
      "out: torch.Size([30, 200, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 30\n",
    "max_sequence_length = 200\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5\n",
    "\n",
    "x = torch.randn( (batch_size, max_sequence_length, d_model) ) # English sentence positional encoded \n",
    "y = torch.randn( (batch_size, max_sequence_length, d_model) ) # Kannada sentence positional encoded \n",
    "mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
    "out = decoder(x, y, mask)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6f5c6",
   "metadata": {},
   "source": [
    "# Full Transformer Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3756f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd82d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "        scaled = scaled.permute(1, 0, 2, 3)\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.START_TOKEN = START_TOKEN\n",
    "        self.END_TOKEN = END_TOKEN\n",
    "        self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "    def batch_tokenize(self, batch, start_token, end_token):\n",
    "\n",
    "        def tokenize(sentence, start_token, end_token):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start_token:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "            if end_token:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(get_device())\n",
    "    \n",
    "    def forward(self, x, start_token, end_token): # sentence\n",
    "        x = self.batch_tokenize(x, start_token, end_token)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "  \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n",
    "    \n",
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
    "        x = self.sentence_embedding(x, start_token, end_token)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y, mask):\n",
    "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        _y = y.clone()\n",
    "        y = self.self_attention(y, mask=self_attention_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.layer_norm1(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.layer_norm2(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.layer_norm3(y + _y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 language_to_index,\n",
    "                 START_TOKEN,\n",
    "                 END_TOKEN, \n",
    "                 PADDING_TOKEN):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
    "        y = self.sentence_embedding(y, start_token, end_token)\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                max_sequence_length, \n",
    "                kn_vocab_size,\n",
    "                english_to_index,\n",
    "                kannada_to_index,\n",
    "                START_TOKEN, \n",
    "                END_TOKEN, \n",
    "                PADDING_TOKEN\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                y, \n",
    "                encoder_self_attention_mask=None, \n",
    "                decoder_self_attention_mask=None, \n",
    "                decoder_cross_attention_mask=None,\n",
    "                enc_start_token=False,\n",
    "                enc_end_token=False,\n",
    "                dec_start_token=False, # We should make this true\n",
    "                dec_end_token=False): # x, y are batch of sentences\n",
    "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdb6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
